#!/usr/bin/env python

import argparse
import logging
import time
from urlparse import urlparse
from datetime import datetime
from collections import defaultdict
from itertools import cycle

from requests.auth import HTTPProxyAuth

import grequests

logger = logging.getLogger('crawlera-bench')
logger.setLevel(level=logging.ERROR)


def log_to_file(file_path):
    stream_handler = logging.FileHandler(file_path)
    logger.addHandler(stream_handler)


def set_log_level(level):
    logger.setLevel(level=level)


def parse_args():
    p = argparse.ArgumentParser(description="Crawlera benchmarking tool")
    p.add_argument("urlsfile", help="File with URLs to try")
    p.add_argument("-u", dest="user", help="Crawlera username")
    p.add_argument("-p", dest="password", help="Crawlera password")
    p.add_argument("-H", dest="host", help="Crawlera host (default: %(default)s)", default="proxy.crawlera.com")
    p.add_argument("-c", dest="concurrency", type=int, default=100, help="concurrency. Default: %(default)s")
    p.add_argument("-t", dest="timeout", type=int, default=120, help="timeout (in seconds) Default: %(default)s")
    p.add_argument("-i", dest="interval", type=int, default=1, help="report interval. Default: %(default)s")
    p.add_argument("-e", dest="estimate", action="store_true", help="show req/min estimate instead of actual requests")
    p.add_argument(
        "-r", dest="run_time", type=int,
        default=0, help="define time of benchmark run in seconds, 0 unlimited")
    p.add_argument(
        "--ca-cert", dest="certificate",
        help="link to Crawlera CA certificate (allows to use CONNECT method "
             "for https requests). If not provided 'x-crawlera-use-https' "
             "header is used instead of CONNECT",
        default=False,
        )
    p.add_argument("--log-to-file", type=log_to_file,
                   help="Allows to store verbose requests/errors information "
                        " in logfile")
    p.add_argument("--log-level", type=set_log_level, nargs='?',
                   help="Set log level (use INFO to add more information)",
                   const="ERROR")

    return p.parse_args()


def print_header(args):
    print("Host            : %s" % args.host)
    print("Concurrency     : %d" % args.concurrency)
    print("Timeout         : %d sec" % args.timeout)
    print("Report interval : %d sec" % args.interval)
    unit = "requests per minute (est)" if args.estimate else "requests per %d sec" % args.interval
    print("Unit            : %s" % unit)
    print("")
    hdr = "time                netloc                           all   2xx   3xx   4xx   5xx   503   t/o   err  |      minw     maxw"
    print(hdr)


def refresh_stats(allstats):
    for netloc, stats in sorted(allstats.items()):
        stats["netloc"] = netloc
        stats["time"] = datetime.utcnow().replace(microsecond=0)
        print "%(time)16s %(netloc)-30s %(r)5d %(r2)5d %(r3)5d %(r4)5d %(r5)5d %(r503)5d %(t)5d %(e)5d  |   %(minw)7.3f  %(maxw)7.3f" % stats


def main():
    args = parse_args()
    with open(args.urlsfile) as f:
        urls = [line.strip() for line in f]

    print_header(args)

    auth = HTTPProxyAuth(args.user, args.password)
    start_time = time.time()
    proxies = {"https": "http://%s:8010/" % args.host,
               "http": "http://%s:8010/" % args.host}

    t = time.time()
    urls_iterator = cycle(urls)
    allstats = defaultdict(lambda: defaultdict(int))
    
    def exception_handler(request, exception):
        logger.error("request {} encountered exception {}".format(request, exception))
    
    while t - start_time < args.run_time:
        t = time.time()
        reqs = []
        for i in range(args.concurrency):
            url = urls_iterator.next()
            headers = {}
            if not args.certificate and url.startswith("https"):
                url = url.replace("https://", "http://", 1)
                headers["x-crawlera-use-https"] = 1

            req = grequests.get(url,
                                headers=headers,
                                proxies=proxies,
                                timeout=args.timeout,
                                auth=auth,
                                allow_redirects=False)

            reqs.append(req)

        responses = grequests.map(reqs, exception_handler=exception_handler)
        for r in responses:
            netloc = urlparse(url).netloc  # TODO: we should get this from crawlera
            allstats[netloc]["r{}".format(r.status_code / 100)] += 1
        refresh_stats(allstats)

    print("Stopping the benchmark", args.run_time)
    print("Start time", start_time, t-start_time)


if __name__ == "__main__":
    main()
