#!/usr/bin/env python

import sys, argparse, logging, requests, time, os
from urlparse import urlparse
from datetime import datetime
from collections import defaultdict
from threading import Thread, Lock
from Queue import Queue
from requests.auth import HTTPProxyAuth

logger = logging.getLogger('crawlera-bench')


def setup_logger(file_path):
    logging.getLogger('crawlera-bench').setLevel(level=logging.ERROR)
    stream_handler = logging.FileHandler(file_path)
    logger.addHandler(stream_handler)


def parse_args():
    p = argparse.ArgumentParser(description="Crawlera benchmarking tool")
    p.add_argument("urlsfile", help="File with URLs to try")
    p.add_argument("-u", dest="user", help="Crawlera username")
    p.add_argument("-p", dest="password", help="Crawlera password")
    p.add_argument("-H", dest="host", help="Crawlera host (default: %(default)s)", default="proxy.crawlera.com")
    p.add_argument("-c", dest="concurrency", type=int, default=100, help="concurrency. Default: %(default)s")
    p.add_argument("-t", dest="timeout", type=int, default=120, help="timeout (in seconds) Default: %(default)s")
    p.add_argument("-i", dest="interval", type=int, default=1, help="report interval. Default: %(default)s")
    p.add_argument("-e", dest="estimate", action="store_true", help="show req/min estimate instead of actual requests")
    p.add_argument(
        "-r", dest="run_time", type=int,
        default=0, help="define time of benchmark run in seconds, 0 unlimited")
    p.add_argument(
        "--ca-cert", dest="certificate",
        help="link to Crawlera CA certificate (allows to use CONNECT method "
             "for https requests). If not provided 'x-crawlera-use-https' "
             "header is used instead of CONNECT",
        default=False,
        )
    p.add_argument("--add-file-logger", type=setup_logger, nargs='?',
                   help="Allows to store verbose requests/errors information "
                   " in logfile",
                   const="/tmp/benchmark_log")
    return p.parse_args()


def worker_request(
        allstats, statslock, queue, user, password, certificate,
        host, timeout, run_time, start_time, m):
    auth = HTTPProxyAuth(user, password)
    proxies = {"https": "http://%s:8010/" % host,
               "http": "http://%s:8010/" % host}
    while True:
        headers = {}
        url = queue.get()
        netloc = urlparse(url).netloc  # TODO: we should get this from crawlera
        stats = allstats[netloc]
        t = time.time()

        # Exit bench after a given amount of time.
        if run_time is not 0 and t - start_time > run_time:
            print "Stopping the benchmark", run_time
            print "Start time", start_time, t-start_time
            os._exit(1)

        if not certificate and url.startswith("https"):
            url = url.replace("https://", "http://", 1)
            headers["x-crawlera-use-https"] = 1

        try:
            logger.info("Requesting: url:{}, headers: {}, cert: {}"
                        .format(url, headers, certificate))
            r = requests.get(
                url,
                headers=headers,
                proxies=proxies,
                timeout=timeout,
                auth=auth,
                allow_redirects=False,
                verify=certificate)
            if r.status_code == 503:
                rk = 'r503'
                logger.error("503 from url:{}, headers: {}, cert: {}"
                             .format(url, headers, certificate))
            else:
                rk = 'r%d' % (r.status_code / 100)
            with statslock:
                stats[rk] += m
        except requests.Timeout:
            stats["t"] += m
            logger.error("Timeout from url:{}, headers: {}, cert: {}"
                         .format(url, headers, certificate))
        except Exception:
            stats["e"] += m
            logger.error("{} from url:{}, headers: {}, cert: {}"
                         .format(sys.exc_info(), url, headers, certificate))
        finally:
            wait = time.time() - t
            with statslock:
                stats["r"] += m
                stats["minw"] = min(wait, stats.get("minw", sys.maxint))
                stats["maxw"] = max(wait, stats["maxw"])
            queue.task_done()


def dumpstats(allstats, statslock, args):
    print "Host            : %s" % args.host
    print "Concurrency     : %d" % args.concurrency
    print "Timeout         : %d sec" % args.timeout
    print "Report interval : %d sec" % args.interval
    unit = "requests per minute (est)" if args.estimate else "requests per %d sec" % args.interval
    print "Unit            : %s" % unit
    print
    hdr = "time                netloc                           all   2xx   3xx   4xx   5xx   503   t/o   err  |      minw     maxw"
    print hdr
    while True:
        time.sleep(args.interval)
        with statslock:
            for netloc, stats in sorted(allstats.items()):
                stats["netloc"] = netloc
                stats["time"] = datetime.utcnow().replace(microsecond=0)
                print "%(time)16s %(netloc)-30s %(r)5d %(r2)5d %(r3)5d %(r4)5d %(r5)5d %(r503)5d %(t)5d %(e)5d  |   %(minw)7.3f  %(maxw)7.3f" % stats
                stats.clear()


def main():
    start_time = time.time()
    args = parse_args()
    allstats = defaultdict(lambda: defaultdict(int))
    statslock = Lock()
    queue = Queue(maxsize=1)

    m = 60 / float(args.interval) if args.estimate else 1

    for _ in range(args.concurrency):
        t = Thread(
            target=worker_request,
            args=(
                allstats, statslock, queue, args.user, args.password,
                args.certificate, args.host, args.timeout,
                args.run_time, start_time, m))
        t.daemon = True
        t.start()
    t = Thread(target=dumpstats, args=(allstats, statslock, args))
    t.daemon = True
    t.start()
    with open(args.urlsfile) as f:
        for line in f:
            queue.put(line.rstrip())
    queue.join()

if __name__ == "__main__":
    main()
